{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Extracts emme data into csv's for further processing and analysis\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "import inro.modeller as _m\n",
    "import inro.emme.datatable as _dt\n",
    "import inro.emme.database as _db\n",
    "import inro.emme.desktop.worksheet as _ws\n",
    "import inro.emme.desktop.app as _ap\n",
    "\n",
    "start_t = time()\n",
    "\n",
    "mm = _m.Modeller()\n",
    "desktop = mm.desktop\n",
    "de = desktop.data_explorer()\n",
    "\n",
    "imp_nwp = mm.tool('tmg.input_output.import_network_package')\n",
    "imp_mat = mm.tool('tmg.input_output.import_binary_matrix')\n",
    "exp_mat = mm.tool('inro.emme.data.matrix.export_matrices')\n",
    "ext_att = mm.tool('inro.emme.data.extra_attribute.create_extra_attribute')\n",
    "shortest_path = mm.tool('inro.emme.network_calculation.shortest_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "# USER INPUTS\n",
    "# run specs\n",
    "# update/check this cell before running\n",
    "\n",
    "run_properties = {\n",
    "    \n",
    "    1:{\n",
    "        'scen_start': 1,\n",
    "        'mat_start': 1,\n",
    "        'run_name': 'Base_2016',\n",
    "        'run_date': '20210707',\n",
    "        'scen_folder': r'2016 Base Lakeshore TPAP Final - July 7, 2021'\n",
    "    },\n",
    "\n",
    "    2:{\n",
    "        'scen_start': 11,\n",
    "        'mat_start': 51,\n",
    "        'run_name': 'Scen1_2041',\n",
    "        'run_date': '20210712',\n",
    "        'scen_folder': r'2041 Scen 1 Lakeshore TPAP Final - July 12th, 2021'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'scen_start': 21,\n",
    "        'mat_start': 101,\n",
    "        'run_name': 'Scen2a_2041',\n",
    "        'run_date': '20210712',\n",
    "        'scen_folder': r'2041 Scen 2a Lakeshore TPAP Final - July 12th, 2021'\n",
    "    },\n",
    "    \n",
    "    4:{\n",
    "        'scen_start': 31,\n",
    "        'mat_start': 151,\n",
    "        'run_name': 'Scen2b_2041',\n",
    "        'run_date': '20210712',\n",
    "        'scen_folder': r'2041 Scen 2b Lakeshore TPAP Final - July 12th, 2021'\n",
    "    },\n",
    "    5:{\n",
    "        'scen_start': 41,\n",
    "        'mat_start': 201,\n",
    "        'run_name': 'Scen0_2041',\n",
    "        'run_date': '20210803',\n",
    "        'scen_folder': r'2041 Scen 0 Lakeshore TPAP Final - Aug 3rd, 2021'\n",
    "    },\n",
    "    6:{\n",
    "        'scen_start': 51,\n",
    "        'mat_start': 251,\n",
    "        'run_name': 'Scen3_2041',\n",
    "        'run_date': '20210909',\n",
    "        'scen_folder': r'2041 Scen 3 Lakeshore TPAP Final - Sept 9, 2021'\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# True or False, run or don't run\n",
    "process_nwps = False\n",
    "process_matrices = False\n",
    "process_auto_metrics = True\n",
    "process_transit_metrics = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant specs (they don't change from run to run)\n",
    "\n",
    "# directories\n",
    "\n",
    "data_dir = r'C:\\Users\\ANLIU\\OneDrive - HDR, Inc\\Documents\\5.0_Project_Dev\\5.2.3 Traffic - Transit\\Macro Model Processing\\0 Model Outputs'\n",
    "out_dir = r'C:\\Users\\ANLIU\\OneDrive - HDR, Inc\\Documents\\5.0_Project_Dev\\5.2.3 Traffic - Transit\\Macro Model Processing\\1 Processed Outputs'\n",
    "\n",
    "emmebank_dir = os.path.dirname(_m.Modeller().emmebank.path)\n",
    "\n",
    "# period specs\n",
    "with open(os.path.join(out_dir, 'period_specs.json')) as period_specs_file:\n",
    "    periods = json.load(period_specs_file)\n",
    "\n",
    "period_order = [\n",
    "    'AM',\n",
    "    'MD',\n",
    "    'PM',\n",
    "    'EV',\n",
    "    'ON'\n",
    "]\n",
    "\n",
    "# nodes along lakeshore by scenario\n",
    "with open(os.path.join(out_dir, 'lakeshore_nodes.json')) as lakeshore_nodes_file:\n",
    "    lakeshore_nodes_sets = json.load(lakeshore_nodes_file)\n",
    "\n",
    "# nodes and zones in scope\n",
    "\n",
    "inscope_nodes = pd.read_csv(os.path.join(out_dir,'inscope_nodes.csv'))\n",
    "inscope_nodes = inscope_nodes.node.tolist()\n",
    "inscope_zones = pd.read_csv(os.path.join(out_dir,'inscope_zones.csv'))\n",
    "inscope_zones = inscope_zones.zone.tolist()\n",
    "\n",
    "# screenlines\n",
    "screenlines = pd.read_csv(os.path.join(out_dir,'screenlines.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "# the actual calculations and stuff\n",
    "\n",
    "def process_outputs(run_properties):\n",
    "    \n",
    "    # run properties\n",
    "    scen_start = run_properties['scen_start']\n",
    "    mat_start = run_properties['mat_start']\n",
    "    run_name = run_properties['run_name']\n",
    "    run_date = run_properties['run_date']\n",
    "    scen_folder = run_properties['scen_folder']\n",
    "    \n",
    "    lakeshore_nodes = lakeshore_nodes_sets[run_name]\n",
    "    \n",
    "    # output folder for this run\n",
    "    processed_dir = os.path.join(out_dir,run_name + ' ' + run_date)\n",
    "    if not os.path.isdir(processed_dir):\n",
    "        os.mkdir(processed_dir) \n",
    "    \n",
    "    # scenrio-specific input directories\n",
    "    nwp_dir = os.path.join(data_dir, scen_folder, 'FinalNetworks')\n",
    "    mat_dir_LOS = os.path.join(data_dir, scen_folder, 'LOS Matrices')        \n",
    "    mat_dir_dem = os.path.join(data_dir, scen_folder, 'Demand')\n",
    "\n",
    "    # import network packages\n",
    "\n",
    "    if process_nwps:\n",
    "\n",
    "        nwp_files = [\n",
    "            'AMRoad.nwp',\n",
    "            'AMTransit.nwp',\n",
    "            'MDRoad.nwp',\n",
    "            'MDTransit.nwp',\n",
    "            'PMRoad.nwp',\n",
    "            'PMTransit.nwp',\n",
    "            'EVRoad.nwp',\n",
    "            'EVTransit.nwp',\n",
    "            'ONRoad.nwp',\n",
    "        ]\n",
    "\n",
    "        scen = scen_start\n",
    "\n",
    "        for f in nwp_files:\n",
    "            'Processing nwp for {}'.format(f)\n",
    "            \n",
    "            nwp_file = os.path.join(nwp_dir,f)\n",
    "            imp_nwp.NetworkPackageFile = nwp_file\n",
    "            imp_nwp.ScenarioId = scen\n",
    "            imp_nwp.OverwriteScenarioFlag = True\n",
    "            imp_nwp.ScenarioName = run_name + f[:len(f)-4]\n",
    "            imp_nwp.ScenarioDescription = run_name + '_' + run_date + '_' + f[:len(f)-4]\n",
    "\n",
    "            imp_nwp.run()\n",
    "\n",
    "            scen += 1\n",
    "\n",
    "        app = _ap.connect()\n",
    "        app.refresh_data()\n",
    "        \n",
    "    # import matrices and save to file\n",
    "\n",
    "    if process_matrices:\n",
    "        \n",
    "        mat_no = mat_start # count start (each run has 36)\n",
    "\n",
    "        mat_files_LOS = [\n",
    "            'acost',\n",
    "            'aivtt',\n",
    "            'tivtt',\n",
    "            'twait',\n",
    "            'twalk'\n",
    "        ]\n",
    "        \n",
    "        mat_files_dem = [\n",
    "            'AutoMatrix',\n",
    "            'BicycleMatrix',\n",
    "            'TransitMatrix',\n",
    "            'WalkMatrix',\n",
    "            'LightMatrix',\n",
    "            'MediumMatrix',\n",
    "            'HeavyMatrix'\n",
    "        ]\n",
    "\n",
    "        scen = scen_start\n",
    "\n",
    "        for p in period_order:\n",
    "\n",
    "            scenario_id = scen\n",
    "            emmebank = inro.modeller.Modeller().emmebank\n",
    "            scenario = emmebank.scenario(scenario_id)\n",
    "\n",
    "            # set primary scenario\n",
    "            de.replace_primary_scenario(scenario)\n",
    "            \n",
    "            # LOS matrices\n",
    "            for m in mat_files_LOS:\n",
    "                \n",
    "                'Processing LOS matrices for {}, {}'.format(p,m)\n",
    "\n",
    "                if not (p == 'ON' and m[0] == 't'): # no transit in ON\n",
    "\n",
    "                    # import binary matrix\n",
    "                    mat_file = os.path.join(mat_dir_LOS, p, m + '.mtx')\n",
    "                    mat_id = 'mf' + str(mat_no)\n",
    "                    imp_mat.MatrixId = mat_id\n",
    "                    imp_mat.ImportFile = mat_file\n",
    "                    imp_mat.MatrixDescription = run_name + ' ' + run_date + ' ' + p + ' ' + m\n",
    "                    imp_mat.Scenario = scenario\n",
    "                    imp_mat.run()\n",
    "\n",
    "                    # export to text file\n",
    "\n",
    "                    mat_file_out = 'LOSmatrix_'+p + '_' + m + '.csv'\n",
    "                    matrices_file = os.path.join(processed_dir, mat_file_out)\n",
    "                    exp_mat(matrices=mat_id,\n",
    "                        export_file=matrices_file,\n",
    "                        export_format=\"PROMPT_DATA_FORMAT\",\n",
    "                        skip_default_values=False,\n",
    "                        full_matrix_line_format=\"ONE_ENTRY_PER_LINE\",\n",
    "                        scenario = scenario\n",
    "                           )   \n",
    "\n",
    "                    mat_no +=1\n",
    "            \n",
    "            # demand matrices\n",
    "            for m in mat_files_dem:\n",
    "                \n",
    "                'Processing demand matrices for {}, {}'.format(p,m)\n",
    "                \n",
    "                if p!= 'ON':\n",
    "                    \n",
    "                    # import binary matrix\n",
    "                    mat_file = os.path.join(mat_dir_dem, p+m + '.mtx')\n",
    "                    mat_id = 'mf' + str(mat_no)\n",
    "                    imp_mat.MatrixId = mat_id\n",
    "                    imp_mat.ImportFile = mat_file\n",
    "                    imp_mat.MatrixDescription = run_name + ' ' + run_date + ' ' + p + ' ' + m[:-6] + ' demand'\n",
    "                    imp_mat.Scenario = scenario\n",
    "                    imp_mat.run()\n",
    "\n",
    "                    # export to text file\n",
    "\n",
    "                    mat_file_out = 'demand_matrix_'+p + '_' + m[:-6] + '.csv'\n",
    "                    matrices_file = os.path.join(processed_dir, mat_file_out)\n",
    "                    exp_mat(matrices=mat_id,\n",
    "                        export_file=matrices_file,\n",
    "                        export_format=\"PROMPT_DATA_FORMAT\",\n",
    "                        skip_default_values=False,\n",
    "                        full_matrix_line_format=\"ONE_ENTRY_PER_LINE\",\n",
    "                        scenario = scenario\n",
    "                           )   \n",
    "\n",
    "                    mat_no +=1\n",
    "                    \n",
    "            \n",
    "            scen += 2\n",
    "            \n",
    "    # get auto metrics\n",
    "\n",
    "    if process_auto_metrics:\n",
    "\n",
    "        scen = scen_start\n",
    "\n",
    "        vkt_scen = pd.DataFrame(index = periods)\n",
    "\n",
    "\n",
    "        for p in period_order:\n",
    "            \n",
    "            p_factor = periods[p]['factor']\n",
    "            \n",
    "            'Processing auto metics for {}'.format(p)\n",
    "\n",
    "            \n",
    "            emmebank = inro.modeller.Modeller().emmebank\n",
    "            scenario = emmebank.scenario(scen)\n",
    "\n",
    "            # set primary scenario\n",
    "            de.replace_primary_scenario(scenario)\n",
    "\n",
    "            # create attribute for visual check and filtering\n",
    "            check_links = ext_att(extra_attribute_type=\"LINK\",\n",
    "                               extra_attribute_name=\"@check_ls\",\n",
    "                               extra_attribute_description=\"link category for Lakeshore\",\n",
    "                               overwrite=True) \n",
    "            check_nodes = ext_att(extra_attribute_type=\"NODE\",\n",
    "                               extra_attribute_name=\"@lts_area\",\n",
    "                               extra_attribute_description=\"node category for Lakeshore\",\n",
    "                               overwrite=True)\n",
    "            check_nodes = ext_att(extra_attribute_type=\"NODE\",\n",
    "                               extra_attribute_name=\"@iszone\",\n",
    "                               extra_attribute_description=\"for shortest path\",\n",
    "                               overwrite=True)\n",
    "            # screenline attribute\n",
    "            check_links = ext_att(extra_attribute_type=\"LINK\",\n",
    "                               extra_attribute_name=\"@screenline\",\n",
    "                               extra_attribute_description=\"for screenline analysis\",\n",
    "                               overwrite=True) \n",
    "\n",
    "            # get network  \n",
    "            network = scenario.get_network()\n",
    "            \n",
    "            # mark nodes\n",
    "            for n in network.nodes():\n",
    "                if int(n.id) in inscope_nodes:\n",
    "                    n['@lts_area'] = 1\n",
    "                if n.is_centroid:\n",
    "                    n['@iszone'] = 1\n",
    "\n",
    "            # get vkt \n",
    "            vkt = 0\n",
    "            vkt_peel = 0\n",
    "            vkt_ls_area = 0\n",
    "            vkt_lakeshore = 0\n",
    "            for l in network.links():\n",
    "                l_vkt = l.length * l.auto_volume\n",
    "                vkt += l_vkt\n",
    "\n",
    "                if (int(l.i_node.id) >= 40000 and int(l.i_node.id) <= 49999) or (int(l.j_node.id) >= 40000 and int(l.j_node.id) <= 49999): # link in Peel Region\n",
    "                    vkt_peel += l_vkt\n",
    "                    \n",
    "                if int(l.i_node.id) in (inscope_nodes) and int(l.j_node.id) in (inscope_nodes):\n",
    "                    vkt_ls_area += l_vkt\n",
    "                    l['@check_ls'] = 1\n",
    "\n",
    "                if int(l.i_node.id) in (lakeshore_nodes) and int(l.j_node.id) in (lakeshore_nodes):\n",
    "                    vkt_lakeshore += l_vkt\n",
    "                    l['@check_ls'] = 2\n",
    "                    \n",
    "                # assign screenline value\n",
    "                if str(l.id) in screenlines.id.to_list():\n",
    "                    l['@screenline'] = screenlines.loc[screenlines.id == str(l.id), 'screenline']\n",
    "\n",
    "            vkt_scen.loc[p,'Total'] = vkt /p_factor\n",
    "            vkt_scen.loc[p,'Peel'] = vkt_peel /p_factor\n",
    "            vkt_scen.loc[p,'Lakeshore Area'] = vkt_ls_area /p_factor\n",
    "            vkt_scen.loc[p,'Lakeshore Road'] = vkt_lakeshore /p_factor\n",
    "\n",
    "            scenario.publish_network(network)\n",
    "\n",
    "            app = _ap.connect()\n",
    "            app.refresh_data()\n",
    "\n",
    "            # get worksheet for speeds and demand\n",
    "\n",
    "            root_ws_folder = desktop.root_worksheet_folder()\n",
    "            links_sheet = root_ws_folder.find_item(['Lakeshore','Links_Lakeshore'])\n",
    "\n",
    "            links = links_sheet.open()\n",
    "            links.save_as_data_table(name = 'Links_forExport',overwrite = True)\n",
    "            links.close()\n",
    "            dts = desktop.project.data_tables()\n",
    "            links_dt = dts.table('Links_forExport')\n",
    "            data = links_dt.get_data()\n",
    "            output_file = os.path.join(processed_dir,'LinksTable_' + p + '.csv')\n",
    "            data.export_to_csv(output_file, separator = ',')\n",
    "            \n",
    "            # get worksheet for links in study area\n",
    "            \n",
    "            links_sheet = root_ws_folder.find_item(['Lakeshore','Links_StudyArea'])\n",
    "\n",
    "            links = links_sheet.open()\n",
    "            links.save_as_data_table(name = 'Links_forExport',overwrite = True)\n",
    "            links.close()\n",
    "            dts = desktop.project.data_tables()\n",
    "            links_dt = dts.table('Links_forExport')\n",
    "            data = links_dt.get_data()\n",
    "            output_file = os.path.join(processed_dir,'LinksStudyArea_' + p + '.csv')\n",
    "            data.export_to_csv(output_file, separator = ',')\n",
    "            \n",
    "            \n",
    "            # get distance skims\n",
    "            sp_modes = [scenario.mode('c')]\n",
    "            sp_out = os.path.join(processed_dir,'ShortestDistance_' + p)\n",
    "            sp = shortest_path(\n",
    "                modes = sp_modes,\n",
    "                roots_attribute=\"@iszone\",\n",
    "                leafs_attribute=\"@iszone\",\n",
    "                link_cost_attribute=\"length\",\n",
    "                num_processors=\"max-1\",\n",
    "                direction=\"AUTO\",\n",
    "                costs_file=sp_out,\n",
    "                return_numpy=False)\n",
    "\n",
    "\n",
    "            scen += 2 #skip transit scenarios for auto metrics\n",
    "\n",
    "\n",
    "        vkt_scen.to_csv(os.path.join(processed_dir, 'vkt.csv'), index = True)\n",
    "        \n",
    "    # get transit metrics\n",
    "    if process_transit_metrics: \n",
    "        \n",
    "        scen = scen_start + 1 # skip auto\n",
    "\n",
    "        pkt_transit_scen = pd.DataFrame(index = periods)\n",
    "\n",
    "        for p in period_order: \n",
    "            \n",
    "            if p != 'ON': # omit ON\n",
    "                \n",
    "#                 p_hours = (periods[p]['end'] - periods[p]['start'] ) / 60 \n",
    "            \n",
    "                'Processing transit metics for {}'.format(p)\n",
    "\n",
    "                scenario_id = scen\n",
    "                emmebank = inro.modeller.Modeller().emmebank\n",
    "                scenario = emmebank.scenario(scenario_id)\n",
    "\n",
    "                # set primary scenario\n",
    "                de.replace_primary_scenario(scenario)\n",
    "\n",
    "                # create attribute for visual check and filtering\n",
    "                check_links = ext_att(extra_attribute_type=\"TRANSIT_LINE\",\n",
    "                                   extra_attribute_name=\"@check_ls\",\n",
    "                                   extra_attribute_description=\"line category for Lakeshore\",\n",
    "                                   overwrite=True)    \n",
    "                # get network\n",
    "                network = scenario.get_network()  \n",
    "\n",
    "                # get pkt transit\n",
    "                pkt_total = 0\n",
    "                pkt_peel = 0\n",
    "                pkt_line23 = 0\n",
    "                pkt_express = 0\n",
    "                pkt_line14 = 0\n",
    "                pkt_line8 = 0\n",
    "                pkt_line45 = 0\n",
    "                pkt_line29 = 0\n",
    "                pkt_line13 = 0\n",
    "                pkt_line111 = 0\n",
    "                pkt_line185 = 0\n",
    "                pkt_line2 = 0\n",
    "                pkt_hur = 0\n",
    "\n",
    "                # mark Peel transit\n",
    "                for li in network.transit_lines():\n",
    "                    if li.id[0] == 'M' or li.id[0] == 'B':\n",
    "                        li['@check_ls'] = 1\n",
    "\n",
    "                for ts in network.transit_segments():\n",
    "\n",
    "                    if ts.j_node != None:\n",
    "                        ts_pkt = ts.transit_volume * ts.link.length\n",
    "                        pkt_total += ts_pkt\n",
    "\n",
    "                        if ts.line['@check_ls'] == 1:\n",
    "                            pkt_peel += ts_pkt\n",
    "\n",
    "                        if 'MLak' in ts.line.id:\n",
    "                            pkt_express += ts_pkt\n",
    "\n",
    "                        if 'M023' in ts.line.id:\n",
    "                            pkt_line23 += ts_pkt\n",
    "\n",
    "                        if 'M014' in ts.line.id:\n",
    "                            pkt_line14 += ts_pkt\n",
    "\n",
    "                        if 'M008' in ts.line.id:\n",
    "                            pkt_line8 += ts_pkt\n",
    "                            \n",
    "                        if 'M045' in ts.line.id:\n",
    "                            pkt_line45 += ts_pkt\n",
    "                            \n",
    "                        if 'M029' in ts.line.id:\n",
    "                            pkt_line29 += ts_pkt\n",
    "                            \n",
    "                        if 'M013' in ts.line.id:\n",
    "                            pkt_line13 += ts_pkt\n",
    "                            \n",
    "                        if 'M111' in ts.line.id:\n",
    "                            pkt_line111 += ts_pkt\n",
    "                            \n",
    "                        if 'M185' in ts.line.id:\n",
    "                            pkt_line185 += ts_pkt\n",
    "                            \n",
    "                        if 'M002' in ts.line.id:\n",
    "                            pkt_line2 += ts_pkt\n",
    "                            \n",
    "                        if 'MHur' in ts.line.id:\n",
    "                            pkt_hur += ts_pkt\n",
    "\n",
    "\n",
    "\n",
    "                    pkt_transit_scen.loc[p,'Total'] = pkt_total \n",
    "                    pkt_transit_scen.loc[p,'Peel'] = pkt_peel \n",
    "                    pkt_transit_scen.loc[p,'Line23'] = pkt_line23 \n",
    "                    pkt_transit_scen.loc[p,'LakeshoreExpress'] = pkt_express \n",
    "                    pkt_transit_scen.loc[p,'Line14'] = pkt_line14 \n",
    "                    pkt_transit_scen.loc[p,'Line8'] = pkt_line8 \n",
    "                    pkt_transit_scen.loc[p,'Line45'] = pkt_line45 \n",
    "                    pkt_transit_scen.loc[p,'Line29'] = pkt_line29 \n",
    "                    pkt_transit_scen.loc[p,'Line13'] = pkt_line13 \n",
    "                    pkt_transit_scen.loc[p,'Line111'] = pkt_line111 \n",
    "                    pkt_transit_scen.loc[p,'Line185'] = pkt_line185 \n",
    "                    pkt_transit_scen.loc[p,'Line2'] = pkt_line2 \n",
    "                    pkt_transit_scen.loc[p,'HuLRT'] = pkt_hur \n",
    "\n",
    "    #                 scenario.publish_network(network)\n",
    "\n",
    "                # get worksheet for speeds and demand\n",
    "                root_ws_folder = desktop.root_worksheet_folder()\n",
    "                tseg_sheet = root_ws_folder.find_item(['Lakeshore','Segments_LS'])\n",
    "\n",
    "                tseg = tseg_sheet.open()\n",
    "                tseg.save_as_data_table(name = 'TransitSegments_forExport',overwrite = True)\n",
    "                tseg.close()\n",
    "                dts = desktop.project.data_tables()\n",
    "                tseg_dt = dts.table('TransitSegments_forExport')\n",
    "                data = tseg_dt.get_data()\n",
    "                output_file = os.path.join(processed_dir,'TransitSegmentsTable_' + p + '.csv')\n",
    "                data.export_to_csv(output_file, separator = ',')\n",
    "\n",
    "                scen += 2\n",
    "\n",
    "        pkt_transit_scen.loc['ON',:] = 0\n",
    "\n",
    "        pkt_transit_scen.to_csv(os.path.join(processed_dir,'pkt_transit.csv'))\n",
    "\n",
    "#     if process_distance_skims:\n",
    "\n",
    "#         'Processing distance skims {}'.format('test')\n",
    "\n",
    "#         scen = scen_start + 1 # skip auto\n",
    "\n",
    "#         emmebank = inro.modeller.Modeller().emmebank\n",
    "#         scenario = emmebank.scenario(scen)\n",
    "\n",
    "#         # set primary scenario\n",
    "#         de.replace_primary_scenario(scenario)\n",
    "\n",
    "#         # create attribute for visual check and filtering\n",
    "#         check_links = ext_att(extra_attribute_type=\"TRANSIT_LINE\",\n",
    "#                            extra_attribute_name=\"@check_ls\",\n",
    "#                            extra_attribute_description=\"line category for Lakeshore\",\n",
    "#                            overwrite=True)    \n",
    "#         # get network\n",
    "#         network = scenario.get_network()  \n",
    "\n",
    "#         # get zones in scenario\n",
    "#         zones_sc = scenario.zone_numbers\n",
    "\n",
    "#         # create matrix of ones, zero diagonal\n",
    "        \n",
    "\n",
    "                \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "show_input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60:9'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in run_properties:\n",
    "    'Processing {}'.format(run_properties[i]['run_name'])\n",
    "    process_outputs(run_properties[i])\n",
    "    \n",
    "end_t = time()\n",
    "rt = end_t - start_t\n",
    "rt_min = int(rt/60)\n",
    "rt_sec = int(rt%60)\n",
    "rt_text = '{}:{}'.format(rt_min, rt_sec)\n",
    "rt_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "modeller": {
   "desktop_app_port": "61241"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
