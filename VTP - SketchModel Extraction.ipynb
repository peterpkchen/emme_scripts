{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Script for VTP Sketch Model\n",
    "## To pull results from base run of GTAM into VTP Sketch Model\n",
    "Peter - 20220316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import inro.modeller as m\n",
    "import inro.emme.matrix as _matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "mm = m.Modeller()    \n",
    "change_matrix = m.Modeller().tool(\"inro.emme.data.matrix.change_matrix_properties\") \n",
    "import_matrices = mm.tool(\"tmg.input_output.import_binary_matrix\")\n",
    "create_matrix =  m.Modeller().tool(\"inro.emme.data.matrix.create_matrix\")\n",
    "matrix_calculator = m.Modeller().tool(\"inro.emme.matrix_calculation.matrix_calculator\")\n",
    "eb = mm.emmebank\n",
    "\n",
    "sov_dist = 'mf180'\n",
    "truck_cost = 'mf190'\n",
    "active_time = 'mf170'\n",
    "active_dem_offpk = 'mf160'\n",
    "active_dem_pk = 'mf150'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Folder\n",
    "runpath = r'\\\\tore-infs01\\Models\\VaughanTMP\\2051_ForecastRuns\\2051_Preferred_new_20211209_1'\n",
    "scen = 10003\n",
    "\n",
    "def import_matrices_from_directory(matrix_folder, matrix_list, extension, scenario):\n",
    "    #imports all matrices in directory and subfolders of the specified extension type\n",
    "    #matrix_list should be in the format:\n",
    "    #   {file_name1: matrix_num1, file_name2: matrix_num2, ...}\n",
    "    import os\n",
    "\n",
    "    for root, dirs, files in os.walk(matrix_folder):\n",
    "        for matrix_file in files:\n",
    "            if matrix_file.endswith(extension):\n",
    "                name = os.path.splitext(matrix_file)[0]\n",
    "                if name in matrix_list:\n",
    "                    matrix_number = int(matrix_list[name][2:])\n",
    "                    import_matrices(4, matrix_number ,os.path.join(root, matrix_file),scenario, name.replace(\"skim.\",\"\").replace(\"transit.\",\"\").replace(\"peak\",\"pk\").replace(\" \",\"_\") )\n",
    "                    print \"Imported matrix \" + matrix_list[name]\n",
    "                    change_matrix(matrix = matrix_list[name],\n",
    "                        matrix_name = name.replace(\"skim.\",\"\").replace(\"transit.\",\"\").replace(\"peak\",\"pk\").replace(\" \",\"_\")[:40],\n",
    "                        matrix_description = name)\n",
    "    \n",
    "    #check\n",
    "    for name in matrix_list:\n",
    "        if eb.matrix(matrix_list[name]): \n",
    "            n_matrix = eb.matrix(matrix_list[name]).get_numpy_data(scenario)\n",
    "            if n_matrix.sum() == 0 :\n",
    "                print \"Matrix %s was not imported, or contains no values!\" % name\n",
    "        else:\n",
    "            print \"Matrix %s was not imported!\" % name\n",
    "    print \"Finished Importing Matrices\"\n",
    "\n",
    "\n",
    "def create_directory_dict(directory, file_ext):\n",
    "    direc = directory # Get current working directory\n",
    "    ext = file_ext # Select your file delimiter\n",
    "    file_dict = {} # Create an empty dict\n",
    "    # Select only files with the ext extension\n",
    "    txt_files = [i for i in os.listdir(direc) if os.path.splitext(i)[1] == ext]\n",
    "    \n",
    "    # Iterate over your txt files\n",
    "    for f in txt_files:\n",
    "        file_dict[f] = \"\"\n",
    "        \n",
    "    oldkeys = list(file_dict.keys())\n",
    "    newkeys = [s.replace('.mtx', '') for s in oldkeys]\n",
    "    vals = list(file_dict.values())\n",
    "    new_dict = {k: v for k, v in zip(newkeys, vals)}\n",
    "    return new_dict\n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    return(dict2.update(dict1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import demand matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_folder = os.path.join(runpath, 'Demand')\n",
    "mat_list = create_directory_dict(demand_folder, '.mtx')\n",
    "\n",
    "for matrix_name in mat_list.keys():\n",
    "    mat_list[matrix_name] = eb.available_matrix_identifier('FULL')\n",
    "    create_matrix(matrix_id = mat_list[matrix_name],\n",
    "              matrix_name= matrix_name.replace(\" \",\"_\"),\n",
    "              matrix_description = matrix_name)\n",
    "\n",
    "import_matrices_from_directory(demand_folder, mat_list, \".mtx\", scen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LOS Skims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOS_folder = os.path.join(runpath, 'LOS Matrices/AM')\n",
    "mat_list_2 = create_directory_dict(demand_folder, '.mtx')\n",
    "\n",
    "for matrix_name in mat_list_2.keys():\n",
    "    mat_list_2[matrix_name] = eb.available_matrix_identifier('FULL')\n",
    "    create_matrix(matrix_id = mat_list_2[matrix_name],\n",
    "              matrix_name= matrix_name.replace(\" \",\"_\"),\n",
    "              matrix_description = matrix_name)\n",
    "\n",
    "import_matrices_from_directory(LOS_folder, mat_list_2, \".mtx\", scen)\n",
    "\n",
    "matrix_list = Merge(mat_list, mat_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Transit Access Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_access_time = eb.available_matrix_identifier('FULL')\n",
    "create_matrix(matrix_id = transit_access_time,\n",
    "              matrix_name=\"transit.access.time\",\n",
    "              matrix_description = \"transit.access.time\")\n",
    "\n",
    "exp = matrix_list[twait] + \"+\" + matrix_list[twalk]\n",
    "\n",
    "spec = {\n",
    "    \"type\": \"MATRIX_CALCULATION\",\n",
    "    \"result\": transit_access_time,\n",
    "    \"expression\": exp\n",
    "}\n",
    "matrix_calculator(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Peak period is AM, Offpeak period is MD\n",
    "output_cols = {\n",
    "    \"sov_dist\":sov_dist,\n",
    "    \"sov_time\":matrix_list['aivtt'],\n",
    "    \"ff_time\":matrix_list['MD_aivtt'],\n",
    "    \"active_time\": active_time, \n",
    "    \"sov_cost\":matrix_list['acost'], \n",
    "    \"transit_cost\": matrix_list['tfare'],\n",
    "    \"light_truck_pkdem\":matrix_list['AMLightMatrix'],\n",
    "    \"med_truck_pkdem\":matrix_list['AMMediumMatrix'],\n",
    "    \"heavy_truck_pkdem\":matrix_list['AMHeavyMatrix'],\n",
    "    \"light_truck_offpkdem\":matrix_list['MDLightMatrix'],\n",
    "    \"med_truck_offpkdem\":matrix_list['MDMediumMatrix'],\n",
    "    \"heavy_truck_offpkdem\":matrix_list['MDHeavyMatrix'],\n",
    "    \"transit_access_time\": transit_access_time,\n",
    "    \"transit_time\": matrix_list['tivtt'],\n",
    "    \"active_dem_pk\":active_dem_pk,\n",
    "    \"active_dem_offpk\":active_dem_offpk,\n",
    "    \"sov_dem_pk\":matrix_list['AMAutoMatrix'],\n",
    "    \"sov_dem_offpk\":matrix_list['MDAutoMatrix'],\n",
    "    \"transit_dem_pk\":matrix_list['AMTransitMatrix'],\n",
    "    \"transit_dem_offpk\":matrix_list['MDTransitMatrix'],\n",
    "    \"truck_cost\": truck_cost\n",
    "}\n",
    "\n",
    "# posted speed to 60, set \n",
    "\n",
    "first = True\n",
    "indices = eb.matrix(matrix_list[sov_demand_pk]).get_data(scen).indices\n",
    "for col in output_cols:\n",
    "    mat_np = eb.matrix(output_cols[col]).get_numpy_data(scen)\n",
    "    if first:\n",
    "        df_full = pd.DataFrame(mat_np,indices[0],indices[1]).stack().reset_index().rename(columns = {\"level_0\": \"O\",\n",
    "                                                                                           \"level_1\": \"D\",\n",
    "                                                                                          0: col})\n",
    "        first = False\n",
    "    else:\n",
    "        df = pd.DataFrame(mat_np,indices[0],indices[1]).stack().reset_index().rename(columns = {\"level_0\": \"O\",\n",
    "                                                                                           \"level_1\": \"D\",\n",
    "                                                                                          0: col})\n",
    "        df_full = df_full.merge(df,on = [\"O\",\"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.sov_cost = df_full.sov_cost/100\n",
    "df_full.hov_cost = df_full.hov_cost/100\n",
    "df_full[\"dem_truck_pk\"] = df_full.light_truck_pkdem + df_full.med_truck_pkdem + df_full.heavy_truck_pkdem\n",
    "df_full[\"dem_truck_offpk\"] = df_full.light_truck_offpkdem + df_full.med_truck_offpkdem + df_full.heavy_truck_offpkdem \n",
    "df_full[\"dem_pk\"] = df_full.sov_dem_pk + df_full.hov_dem_pk+df_full.transit_dem_pk+df_full.active_dem_pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand_weighted = {u'active_time':u'active_dem_pk',u'ff_time':u'sov_dem_pk',u'sov_time':u'sov_dem_pk',\n",
    "#                    u'hov_cost':u'hov_dem_pk',u'sov_dist':u'sov_dem_pk',u'hov_time':u'sov_dem_pk',\n",
    "#                    u'sov_cost':u'sov_dem_pk'}\n",
    "\n",
    "demand_weighted = {\n",
    "    u'active_time':\"dem_pk\",u'ff_time':\"dem_pk\",u'sov_time':\"dem_pk\",\n",
    "    u'hov_cost':\"dem_pk\",u'sov_dist':\"dem_pk\",u'hov_time':\"dem_pk\",\n",
    "    u'sov_cost':\"dem_pk\",u'transit_cost':\"dem_pk\",u'transit_time':\"dem_pk\",\n",
    "    \"truck_cost\":\"dem_truck_pk\",\"transit_access_time\":\"dem_pk\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for met in demand_weighted.keys():\n",
    "    df_full[met + \"_dem_weight\"] = df_full[met]*df_full[demand_weighted[met]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = r\"C:\\Users\\pechen\\Desktop\\VTP\\SkM\"\n",
    "megazones_df = pd.read_csv(os.path.join(pth,\"TAZ_SketchModelZones_20201110.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal = df_full[(df_full.O <= 6000) & (df_full.D <= 6000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal = df_internal.merge(megazones_df, left_on = \"O\", right_on = \"TAZ\").drop(labels = \"TAZ\",axis = 1).rename(columns = {\"Megazone\": \"Megazone_O\"})\n",
    "df_internal = df_internal.merge(megazones_df, left_on = \"D\", right_on = \"TAZ\").drop(labels = \"TAZ\",axis = 1).rename(columns = {\"Megazone\": \"Megazone_D\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(list):\n",
    "    return sum(list)/len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(list):\n",
    "    return (max(list) + min(list))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_columns = [u'active_time_dem_weight',u'ff_time_dem_weight',u'sov_time_dem_weight',u'sov_cost_dem_weight',\n",
    "                u'hov_cost_dem_weight',u'sov_dist_dem_weight',u'hov_time_dem_weight',\n",
    "                u'active_dem_offpk', u'transit_dem_pk', u'active_dem_pk', u'transit_dem_offpk',\n",
    "                u'hov_dem_offpk', u'sov_dem_offpk', u'dem_truck_pk', u'dem_truck_offpk', u'hov_dem_pk', u'sov_dem_pk',\"dem_pk\",\n",
    "                u'transit_cost_dem_weight',u'transit_time_dem_weight',\"truck_cost_dem_weight\",\"transit_access_time_dem_weight\"]\n",
    "megazone_df = pd.pivot_table(df_internal,values = pivot_columns, index = [\"Megazone_O\",\"Megazone_D\"],aggfunc = sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for met in demand_weighted.keys():\n",
    "    megazone_df[met] = megazone_df[met + \"_dem_weight\"] / megazone_df[demand_weighted[met]]\n",
    "    megazone_df = megazone_df.drop(labels = met + \"_dem_weight\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megazone_df.fillna(0).to_csv(os.path.join(pth, \"2051N1LUV3_MegazoneOutputs_20210225.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megazone_df.fillna(0).to_csv(os.path.join(pth, \"2051N1LUV3_MegazoneOutputs_20210225.csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf7ef214ef546a305a6965a1f2dbfa2a98a8707f3bf9715720dae3c774c165bc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
