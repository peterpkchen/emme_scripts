{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below cell must be run for any extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "import inro.modeller as m\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "mm = m.Modeller()\n",
    "eb = mm.emmebank\n",
    "import inro.emme.matrix as _matrix\n",
    "\n",
    "matrix_calculator = m.Modeller().tool(\"inro.emme.matrix_calculation.matrix_calculator\")\n",
    "create_matrix =  m.Modeller().tool(\"inro.emme.data.matrix.create_matrix\")\n",
    "change_matrix = m.Modeller().tool(\"inro.emme.data.matrix.change_matrix_properties\") \n",
    "import_matrices = mm.tool(\"tmg.input_output.import_binary_matrix\")  \n",
    "matrix_transaction = mm.tool('inro.emme.data.matrix.matrix_transaction')\n",
    "create_extra = m.Modeller().tool(\"inro.emme.data.extra_attribute.create_extra_attribute\")\n",
    "network_calc= m.Modeller().tool(\"inro.emme.network_calculation.network_calculator\")\n",
    "export_matrices = mm.tool(\"tmg.input_output.export_binary_matrix\")  \n",
    "\n",
    "\n",
    "def import_matrices_from_directory(matrix_folder, matrix_list, extension, scenario):\n",
    "    #imports all matrices in directory and subfolders of the specified extension type\n",
    "    #matrix_list should be in the format:\n",
    "    #   {file_name1: matrix_num1, file_name2: matrix_num2, ...}\n",
    "    import os\n",
    "\n",
    "    for root, dirs, files in os.walk(matrix_folder):\n",
    "        for matrix_file in files:\n",
    "            if matrix_file.endswith(extension):\n",
    "                name = os.path.splitext(matrix_file)[0]\n",
    "                if name in matrix_list:\n",
    "                    matrix_number = int(matrix_list[name][2:])\n",
    "                    import_matrices(4, matrix_number ,os.path.join(root, matrix_file),scenario, name.replace(\"skim.\",\"\").replace(\"transit.\",\"\").replace(\"peak\",\"pk\").replace(\" \",\"_\") )\n",
    "                    print \"Imported matrix \" + matrix_list[name]\n",
    "                    change_matrix(matrix = matrix_list[name],\n",
    "                        matrix_name = name.replace(\"skim.\",\"\").replace(\"transit.\",\"\").replace(\"peak\",\"pk\").replace(\" \",\"_\")[:40],\n",
    "                        matrix_description = name)\n",
    "    \n",
    "    #check\n",
    "    for name in matrix_list:\n",
    "        if eb.matrix(matrix_list[name]): \n",
    "            n_matrix = eb.matrix(matrix_list[name]).get_numpy_data(scenario)\n",
    "            if n_matrix.sum() == 0 :\n",
    "                print \"Matrix %s was not imported, or contains no values!\" % name\n",
    "        else:\n",
    "            print \"Matrix %s was not imported!\" % name\n",
    "    print \"Finished Importing Matrices\"\n",
    "\n",
    "def mapTripPurpose(purpose):\n",
    "    purpose_list = purpose.split(\"-\")\n",
    "    purpose_o = purpose_list[0]\n",
    "    purpose_d = purpose_list[1]\n",
    "    work = [\"PrimaryWork\",\"SecondaryWork\"]\n",
    "    market = [\"Market\",\"JointMarket\"]\n",
    "    other = [\"IndividualOther\",\"JointOther\"]\n",
    "    if (purpose_o == \"Home\" or purpose_d == \"Home\") and (purpose_o in work or purpose_d in work):\n",
    "        return \"1 - HBW\"\n",
    "    elif (purpose_o == \"Home\" or purpose_d == \"Home\") and (purpose_o == \"School\" or purpose_d == \"School\"):\n",
    "        return \"2 - HBS\"\n",
    "    elif (purpose_o == 'WorkBasedBusiness' or purpose_d == 'WorkBasedBusiness'):\n",
    "        return \"3 - WBB\"\n",
    "    elif (purpose_d in market):\n",
    "        return \"4 - Market\"\n",
    "    elif (purpose_d in other):\n",
    "        return \"5 - Other\"\n",
    "    elif (purpose_o in market):\n",
    "        return \"4 - Market\"\n",
    "    elif (purpose_o in other):\n",
    "        return \"5 - Other\"\n",
    "    else:\n",
    "        return \"6 - Remaining\"\n",
    "def specificPurpose(purpose):\n",
    "    purpose_list = purpose.split(\"-\")\n",
    "    purpose_o = purpose_list[0]\n",
    "    purpose_d = purpose_list[1]\n",
    "    work = [\"PrimaryWork\",\"SecondaryWork\"]\n",
    "    market_avoid_d = ['IndividualOther','JointOther','JointMarket']\n",
    "    other_avoid_d = ['Market','JointOther','JointMarket']\n",
    "    market_avoid_o = ['JointOther','JointMarket']\n",
    "    other_avoid_o = ['JointOther','JointMarket']\n",
    "    \n",
    "    if (purpose_o in work and purpose_d == 'WorkBasedBusiness'):\n",
    "        return \"1 - WBB\"\n",
    "    elif (purpose_o == \"Market\" and purpose_d not in market_avoid_d) or (purpose_d == \"Market\" and purpose_o not in market_avoid_o):\n",
    "        return \"2 - Market\"\n",
    "    elif (purpose_o == \"IndividualOther\" and purpose_d not in other_avoid_d) or (purpose_d == \"IndividualOther\" and purpose_o not in other_avoid_o):\n",
    "        return \"3 - Other\"\n",
    "    else:\n",
    "        return \"4 - Remaining\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input in cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_directory = r\"C:\\Users\\BCrane\\Documents\\XTMF\\Projects\\Durham Update\" #Path to XTMF Project Directory\n",
    "run_folder = r\"Durham Full Run 2016 - DRT Half Boarding Penalty - 20210519\" # Name of run being processed\n",
    "output_nme = \"model_run_name_test\" #Name of run that will be added to all processed outputs\n",
    "input_pth = r\"C:\\Users\\bcrane\\Documents\\Durham\" #Directory where support files for output extraction are saved\n",
    "output_pth = r\"C:\\Users\\bcrane\\Documents\\Durham\" #Directory where you want processed model outputs to be saved\n",
    "zone_map = pd.read_csv(os.path.join(input_pth,\"GTA_Durham_Zone_Mapping.csv\")) # This file must be located in input_pth directory\n",
    "scen_num = 400 #Scenario must have same zone system as run being extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Output Extraction Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_path = os.path.join(run_directory,run_folder)\n",
    "zone_map_min = zone_map[[\"DTAZ\",\"pd_new\",\"pd_old\", \"pd_tts\"]]\n",
    "scenario = eb.scenario(scen_num)\n",
    "microsim_location = r\"Microsim Results\"\n",
    "microsim_path = os.path.join(run_path, microsim_location)\n",
    "#PORPOW Model\n",
    "porpow_location = r\"Validation\\PoRPoW\\Model\"\n",
    "porpow_path = os.path.join(run_path,porpow_location)\n",
    "import_mat = \"mf50\"\n",
    "first = True\n",
    "for root,dirs,files in os.walk(porpow_path):\n",
    "    for matrix_file in files:\n",
    "        name = os.path.splitext(matrix_file)[0]\n",
    "        matrix_number = int(import_mat[2:])\n",
    "        import_matrices(4, matrix_number ,os.path.join(root, matrix_file),scenario, name)\n",
    "        porpow_mat = eb.matrix(import_mat).get_numpy_data(scenario)\n",
    "        indices = eb.matrix(import_mat).get_data(scen_num).indices\n",
    "        if first:\n",
    "            df_full = pd.DataFrame(porpow_mat,indices[0],indices[1]).stack().reset_index().rename(columns = {\"level_0\": \"TAZ_O\",\n",
    "                                                                                               \"level_1\": \"TAZ_D\",\n",
    "                                                                                              0: name})\n",
    "            first = False\n",
    "        else:\n",
    "            df_curr = pd.DataFrame(porpow_mat,indices[0],indices[1]).stack().reset_index().rename(columns = {\"level_0\": \"TAZ_O\",\n",
    "                                                                                               \"level_1\": \"TAZ_D\",\n",
    "                                                                                              0: name})\n",
    "            df_full = df_full.merge(df_curr, on= [\"TAZ_O\",\"TAZ_D\"])\n",
    "            \n",
    "df_full = df_full.merge(zone_map_min,left_on = \"TAZ_O\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_o\", \"pd_old\":\"pd_old_o\", \"pd_tts\":\"pd_tts_o\"})\n",
    "df_full = df_full.merge(zone_map_min,left_on = \"TAZ_D\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_d\", \"pd_old\":\"pd_old_d\", \"pd_tts\":\"pd_tts_d\"})\n",
    "df_full[\"All\"] = df_full[\"GF\"]+df_full[\"GP\"]+df_full[\"MF\"]+df_full[\"MP\"]+df_full[\"PF\"]+df_full[\"PP\"]+df_full[\"SF\"]+df_full[\"SP\"]\n",
    "all_pivot = pd.pivot_table(df_full[(df_full.pd_tts_o > 0) & (df_full.pd_tts_d > 0)], index = \"pd_tts_o\", columns = \"pd_tts_d\", values = [\"All\"], aggfunc = sum)\n",
    "all_pivot.stack().to_csv(os.path.join(output_pth,\"model_porpow_{}.csv\".format(output_nme)))\n",
    "\n",
    "#Licenses\n",
    "persons_df = pd.read_csv(os.path.join(microsim_path,\"persons.csv\"))\n",
    "households_df = pd.read_csv(os.path.join(microsim_path,\"households.csv\"))\n",
    "persons_df = persons_df.merge(households_df,on=\"household_id\")\n",
    "persons_df = persons_df.merge(zone_map_min,left_on = \"home_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_home\", \"pd_old\":\"pd_old_home\", \"pd_tts\":\"pd_tts_home\"})\n",
    "pot_drivers = persons_df[persons_df[\"age\"] >= 16]\n",
    "license_summary = pd.pivot_table(pot_drivers, index = \"pd_old_home\", columns = \"license\", values = \"weight_x\",aggfunc = sum).fillna(0)\n",
    "license_summary.to_csv(os.path.join(output_pth,\"licensepdsummary_{}.csv\".format(output_nme)))\n",
    "\n",
    "#Auto Ownership\n",
    "def reduce_drivers(drivers):\n",
    "    if drivers <= 3:\n",
    "        return drivers\n",
    "    else:\n",
    "        return 4\n",
    "persons_df[\"license_num\"] = persons_df[\"license\"].map({True: 1, False: 0})\n",
    "license_hh = pd.pivot_table(persons_df, index = \"household_id\", values = \"license_num\",aggfunc = sum).reset_index()\n",
    "household_lic_df = households_df.merge(license_hh, on= \"household_id\").rename(columns = {\"license_num\": \"drivers\"})\n",
    "household_lic_df = household_lic_df.merge(zone_map_min,left_on = \"home_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_home\", \"pd_old\":\"pd_old_home\", \"pd_tts\":\"pd_tts_home\"})\n",
    "household_lic_df[\"driver_mod\"] = household_lic_df[\"drivers\"].map(reduce_drivers)\n",
    "household_lic_summary = pd.pivot_table(household_lic_df, index = [\"driver_mod\",\"pd_old_home\"], columns = \"vehicles\", values = \"weight\", aggfunc = sum).fillna(0)\n",
    "household_lic_summary.to_csv(os.path.join(output_pth,\"householdvehiclesummary_{}.csv\".format(output_nme)))\n",
    "\n",
    "#Episode Generation\n",
    "persons_df = pd.read_csv(os.path.join(microsim_path,\"persons.csv\"))\n",
    "trips_df = pd.read_csv(os.path.join(microsim_path,\"trips.csv\"))\n",
    "durham_trips = trips_df[((trips_df.o_zone >= 1000) & (trips_df.o_zone < 2000))| ((trips_df.d_zone >= 1000) & (trips_df.d_zone < 2000))]\n",
    "durham_trips = durham_trips.merge(persons_df,on=[\"household_id\",\"person_id\"])\n",
    "durham_trips = durham_trips[durham_trips.age >=11]\n",
    "durham_trips[\"combined_act\"] = durham_trips.o_act + \"-\" + durham_trips.d_act\n",
    "durham_trips = durham_trips.merge(zone_map_min,left_on = \"o_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_o\", \"pd_old\":\"pd_old_o\", \"pd_tts\":\"pd_tts_o\"})\n",
    "durham_trips = durham_trips.merge(zone_map_min,left_on = \"d_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_d\", \"pd_old\":\"pd_old_d\", \"pd_tts\":\"pd_tts_d\"})\n",
    "durham_trips[\"Purpose\"] = durham_trips[\"combined_act\"].map(mapTripPurpose)\n",
    "trip_summary = pd.pivot_table(durham_trips,values = \"weight_x\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purpose\", aggfunc = sum, margins = True).fillna(0)\n",
    "\n",
    "trip_summary.to_csv(os.path.join(output_pth,\"dailytrips_episodegen_{}.csv\".format(output_nme)))\n",
    "\n",
    "#Location Choice\n",
    "durham_trips[\"Purp2\"] = durham_trips[\"combined_act\"].map(specificPurpose)\n",
    "trip_summary2 = pd.pivot_table(durham_trips,values = \"weight_x\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purp2\", aggfunc = sum).fillna(0)\n",
    "tripmode_df = pd.read_csv(os.path.join(microsim_path,\"trip_modes.csv\"))\n",
    "durham_trips_time = tripmode_df.merge(durham_trips,how = \"right\", on = ['household_id', 'person_id', 'trip_id'])\n",
    "durham_trips_time[\"weight\"] = durham_trips_time[\"weight\"]/10\n",
    "durham_trips_am = durham_trips_time[(durham_trips_time.o_depart >= 360) & (durham_trips_time.o_depart < 540)]\n",
    "durham_trips_pm = durham_trips_time[(durham_trips_time.o_depart >= 900) & (durham_trips_time.o_depart < 1140)]\n",
    "trip_summary_am = pd.pivot_table(durham_trips_am,values = \"weight\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purp2\", aggfunc = sum).fillna(0).reset_index()\n",
    "trip_summary_pm = pd.pivot_table(durham_trips_pm,values = \"weight\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purp2\", aggfunc = sum).fillna(0).reset_index()\n",
    "trip_summary_am = trip_summary_am.rename(columns= {'1 - WBB': 'AM - 1 - WBB', '2 - Market': 'AM - 2 - Market', \n",
    "                                 '3 - Other': 'AM - 3 - Other', '4 - Remaining': 'AM - 4 - Remaining'})\n",
    "trip_summary_pm = trip_summary_pm.rename(columns= {'1 - WBB': 'PM - 1 - WBB', '2 - Market': 'PM - 2 - Market', \n",
    "                                 '3 - Other': 'PM - 3 - Other', '4 - Remaining': 'PM - 4 - Remaining'})\n",
    "trip_summary2 = trip_summary2.merge(trip_summary_am, on=['pd_tts_o', 'pd_tts_d'], how = \"left\").merge(trip_summary_pm, on=['pd_tts_o', 'pd_tts_d'], how = \"left\").fillna(0)\n",
    "\n",
    "trip_summary2.to_csv(os.path.join(output_pth,\"daily_ampm_locchoicesummary_{}.csv\".format(output_nme)))\n",
    "\n",
    "microsim_location = r\"Microsim Results\"\n",
    "microsim_path = os.path.join(run_path, microsim_location)\n",
    "\n",
    "tripmode_df = pd.read_csv(os.path.join(microsim_path,\"trip_modes.csv\"))\n",
    "\n",
    "#GTA-Wide Mode Share\n",
    "trips_gta = trips_df[(trips_df.o_zone < 6000) & (trips_df.d_zone < 6000)]\n",
    "trips_gta_time = tripmode_df.merge(trips_gta,how = \"right\", on = ['household_id', 'person_id', 'trip_id'])\n",
    "trips_gta_time[\"weight_x\"] = trips_gta_time[\"weight_x\"]/10\n",
    "trips_gta_time[\"weight_z\"] = trips_gta_time[\"weight_x\"]*trips_gta_time[\"weight_y\"]\n",
    "pd.pivot_table(trips_gta_time,columns = \"mode\", values = \"weight_z\", aggfunc = sum).to_csv(os.path.join(output_pth,\"Daily_GTATrips_Mode_{}.csv\".format(output_nme)))\n",
    "\n",
    "#Durham-Specific Mode Share\n",
    "am_modeshare = pd.pivot_table(durham_trips_am,index = [\"pd_tts_o\",\"pd_tts_d\"],columns = \"mode\", values = \"weight\", aggfunc = sum,margins = True).fillna(0)\n",
    "pm_modeshare = pd.pivot_table(durham_trips_pm,index = [\"pd_tts_o\",\"pd_tts_d\"],columns = \"mode\", values = \"weight\", aggfunc = sum,margins = True).fillna(0)\n",
    "am_modeshare.to_csv(os.path.join(output_pth,\"am_modeshare_{}.csv\".format(output_nme)))\n",
    "pm_modeshare.to_csv(os.path.join(output_pth,\"pm_modeshare_{}.csv\".format(output_nme)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PORPOW Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_path = os.path.join(run_directory,run_folder)\n",
    "zone_map_min = zone_map[[\"DTAZ\",\"pd_new\",\"pd_old\", \"pd_tts\"]]\n",
    "scenario = eb.scenario(scen_num)\n",
    "porpow_location = r\"Validation\\PoRPoW\\Model\"\n",
    "porpow_path = os.path.join(run_path,porpow_location)\n",
    "import_mat = \"mf50\"\n",
    "first = True\n",
    "for root,dirs,files in os.walk(porpow_path):\n",
    "    for matrix_file in files:\n",
    "        name = os.path.splitext(matrix_file)[0]\n",
    "        matrix_number = int(import_mat[2:])\n",
    "        import_matrices(4, matrix_number ,os.path.join(root, matrix_file),scenario, name)\n",
    "        porpow_mat = eb.matrix(import_mat).get_numpy_data(scenario)\n",
    "        indices = eb.matrix(import_mat).get_data(scen_num).indices\n",
    "        if first:\n",
    "            df_full = pd.DataFrame(porpow_mat,indices[0],indices[1]).stack().reset_index().rename(columns = {\"level_0\": \"TAZ_O\",\n",
    "                                                                                               \"level_1\": \"TAZ_D\",\n",
    "                                                                                              0: name})\n",
    "            first = False\n",
    "        else:\n",
    "            df_curr = pd.DataFrame(porpow_mat,indices[0],indices[1]).stack().reset_index().rename(columns = {\"level_0\": \"TAZ_O\",\n",
    "                                                                                               \"level_1\": \"TAZ_D\",\n",
    "                                                                                              0: name})\n",
    "            df_full = df_full.merge(df_curr, on= [\"TAZ_O\",\"TAZ_D\"])\n",
    "df_full = df_full.merge(zone_map_min,left_on = \"TAZ_O\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_o\", \"pd_old\":\"pd_old_o\", \"pd_tts\":\"pd_tts_o\"})\n",
    "df_full = df_full.merge(zone_map_min,left_on = \"TAZ_D\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_d\", \"pd_old\":\"pd_old_d\", \"pd_tts\":\"pd_tts_d\"})\n",
    "df_full[\"All\"] = df_full[\"GF\"]+df_full[\"GP\"]+df_full[\"MF\"]+df_full[\"MP\"]+df_full[\"PF\"]+df_full[\"PP\"]+df_full[\"SF\"]+df_full[\"SP\"]\n",
    "all_pivot = pd.pivot_table(df_full[(df_full.pd_tts_o > 0) & (df_full.pd_tts_d > 0)], index = \"pd_tts_o\", columns = \"pd_tts_d\", values = [\"All\"], aggfunc = sum)\n",
    "all_pivot.stack().to_csv(os.path.join(output_pth,\"model_porpow_{}.csv\".format(output_nme)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licenses and Household Vehicle Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_path = os.path.join(run_directory,run_folder)\n",
    "zone_map_min = zone_map[[\"DTAZ\",\"pd_new\",\"pd_old\", \"pd_tts\"]]\n",
    "microsim_location = r\"Microsim Results\"\n",
    "microsim_path = os.path.join(run_path, microsim_location)\n",
    "#Licenses\n",
    "persons_df = pd.read_csv(os.path.join(microsim_path,\"persons.csv\"))\n",
    "households_df = pd.read_csv(os.path.join(microsim_path,\"households.csv\"))\n",
    "persons_df = persons_df.merge(households_df,on=\"household_id\")\n",
    "persons_df = persons_df.merge(zone_map_min,left_on = \"home_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_home\", \"pd_old\":\"pd_old_home\", \"pd_tts\":\"pd_tts_home\"})\n",
    "pot_drivers = persons_df[persons_df[\"age\"] >= 16]\n",
    "license_summary = pd.pivot_table(pot_drivers, index = \"pd_old_home\", columns = \"license\", values = \"weight_x\",aggfunc = sum).fillna(0)\n",
    "license_summary.to_csv(os.path.join(output_pth,\"licensepdsummary_{}.csv\".format(output_nme)))\n",
    "#Auto Ownership\n",
    "def reduce_drivers(drivers):\n",
    "    if drivers <= 3:\n",
    "        return drivers\n",
    "    else:\n",
    "        return 4\n",
    "persons_df[\"license_num\"] = persons_df[\"license\"].map({True: 1, False: 0})\n",
    "license_hh = pd.pivot_table(persons_df, index = \"household_id\", values = \"license_num\",aggfunc = sum).reset_index()\n",
    "household_lic_df = households_df.merge(license_hh, on= \"household_id\").rename(columns = {\"license_num\": \"drivers\"})\n",
    "household_lic_df = household_lic_df.merge(zone_map_min,left_on = \"home_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_home\", \"pd_old\":\"pd_old_home\", \"pd_tts\":\"pd_tts_home\"})\n",
    "household_lic_df[\"driver_mod\"] = household_lic_df[\"drivers\"].map(reduce_drivers)\n",
    "household_lic_summary = pd.pivot_table(household_lic_df, index = [\"driver_mod\",\"pd_old_home\"], columns = \"vehicles\", values = \"weight\", aggfunc = sum).fillna(0)\n",
    "household_lic_summary.to_csv(os.path.join(output_pth,\"householdvehiclesummary_{}.csv\".format(output_nme)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode Generation and Location Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_path = os.path.join(run_directory,run_folder)\n",
    "zone_map_min = zone_map[[\"DTAZ\",\"pd_new\",\"pd_old\", \"pd_tts\"]]\n",
    "microsim_location = r\"Microsim Results\"\n",
    "microsim_path = os.path.join(run_path, microsim_location)\n",
    "\n",
    "#Episode Generation\n",
    "persons_df = pd.read_csv(os.path.join(microsim_path,\"persons.csv\"))\n",
    "trips_df = pd.read_csv(os.path.join(microsim_path,\"trips.csv\"))\n",
    "durham_trips = trips_df[((trips_df.o_zone >= 1000) & (trips_df.o_zone < 2000))| ((trips_df.d_zone >= 1000) & (trips_df.d_zone < 2000))]\n",
    "durham_trips = durham_trips.merge(persons_df,on=[\"household_id\",\"person_id\"])\n",
    "durham_trips = durham_trips[durham_trips.age >=11]\n",
    "durham_trips[\"combined_act\"] = durham_trips.o_act + \"-\" + durham_trips.d_act\n",
    "durham_trips = durham_trips.merge(zone_map_min,left_on = \"o_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_o\", \"pd_old\":\"pd_old_o\", \"pd_tts\":\"pd_tts_o\"})\n",
    "durham_trips = durham_trips.merge(zone_map_min,left_on = \"d_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_d\", \"pd_old\":\"pd_old_d\", \"pd_tts\":\"pd_tts_d\"})\n",
    "durham_trips[\"Purpose\"] = durham_trips[\"combined_act\"].map(mapTripPurpose)\n",
    "trip_summary = pd.pivot_table(durham_trips,values = \"weight_x\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purpose\", aggfunc = sum, margins = True).fillna(0)\n",
    "\n",
    "trip_summary.to_csv(os.path.join(output_pth,\"dailytrips_episodegen_{}.csv\".format(output_nme)))\n",
    "\n",
    "#Location Choice\n",
    "durham_trips[\"Purp2\"] = durham_trips[\"combined_act\"].map(specificPurpose)\n",
    "trip_summary2 = pd.pivot_table(durham_trips,values = \"weight_x\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purp2\", aggfunc = sum).fillna(0)\n",
    "tripmode_df = pd.read_csv(os.path.join(microsim_path,\"trip_modes.csv\"))\n",
    "durham_trips_time = tripmode_df.merge(durham_trips,how = \"right\", on = ['household_id', 'person_id', 'trip_id'])\n",
    "durham_trips_time[\"weight\"] = durham_trips_time[\"weight\"]/10\n",
    "durham_trips_am = durham_trips_time[(durham_trips_time.o_depart >= 360) & (durham_trips_time.o_depart < 540)]\n",
    "durham_trips_pm = durham_trips_time[(durham_trips_time.o_depart >= 900) & (durham_trips_time.o_depart < 1140)]\n",
    "trip_summary_am = pd.pivot_table(durham_trips_am,values = \"weight\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purp2\", aggfunc = sum).fillna(0).reset_index()\n",
    "trip_summary_pm = pd.pivot_table(durham_trips_pm,values = \"weight\", index = [\"pd_tts_o\",\"pd_tts_d\"], columns = \"Purp2\", aggfunc = sum).fillna(0).reset_index()\n",
    "trip_summary_am = trip_summary_am.rename(columns= {'1 - WBB': 'AM - 1 - WBB', '2 - Market': 'AM - 2 - Market', \n",
    "                                 '3 - Other': 'AM - 3 - Other', '4 - Remaining': 'AM - 4 - Remaining'})\n",
    "trip_summary_pm = trip_summary_pm.rename(columns= {'1 - WBB': 'PM - 1 - WBB', '2 - Market': 'PM - 2 - Market', \n",
    "                                 '3 - Other': 'PM - 3 - Other', '4 - Remaining': 'PM - 4 - Remaining'})\n",
    "trip_summary2 = trip_summary2.merge(trip_summary_am, on=['pd_tts_o', 'pd_tts_d'], how = \"left\").merge(trip_summary_pm, on=['pd_tts_o', 'pd_tts_d'], how = \"left\").fillna(0)\n",
    "\n",
    "trip_summary2.to_csv(os.path.join(output_pth,\"daily_ampm_locchoicesummary_{}.csv\".format(output_nme)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTA-Wide Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_path = os.path.join(run_directory,run_folder)\n",
    "zone_map_min = zone_map[[\"DTAZ\",\"pd_new\",\"pd_old\", \"pd_tts\"]]\n",
    "microsim_location = r\"Microsim Results\"\n",
    "microsim_path = os.path.join(run_path, microsim_location)\n",
    "\n",
    "trips_df = pd.read_csv(os.path.join(microsim_path,\"trips.csv\"))\n",
    "tripmode_df = pd.read_csv(os.path.join(microsim_path,\"trip_modes.csv\"))\n",
    "trips_gta = trips_df[(trips_df.o_zone < 6000) & (trips_df.d_zone < 6000)]\n",
    "trips_gta_time = tripmode_df.merge(trips_gta,how = \"right\", on = ['household_id', 'person_id', 'trip_id'])\n",
    "trips_gta_time[\"weight_x\"] = trips_gta_time[\"weight_x\"]/10\n",
    "trips_gta_time[\"weight_z\"] = trips_gta_time[\"weight_x\"]*trips_gta_time[\"weight_y\"]\n",
    "pd.pivot_table(trips_gta_time,columns = \"mode\", values = \"weight_z\", aggfunc = sum).to_csv(os.path.join(output_pth,\"Daily_GTATrips_Mode_{}.csv\".format(output_nme)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durham Specific Peak Period Mode Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "show_input": true
   },
   "outputs": [],
   "source": [
    "run_path = os.path.join(run_directory,run_folder)\n",
    "zone_map_min = zone_map[[\"DTAZ\",\"pd_new\",\"pd_old\", \"pd_tts\"]]\n",
    "microsim_location = r\"Microsim Results\"\n",
    "microsim_path = os.path.join(run_path, microsim_location)\n",
    "\n",
    "persons_df = pd.read_csv(os.path.join(microsim_path,\"persons.csv\"))\n",
    "trips_df = pd.read_csv(os.path.join(microsim_path,\"trips.csv\"))\n",
    "tripmode_df = pd.read_csv(os.path.join(microsim_path,\"trip_modes.csv\"))\n",
    "\n",
    "durham_trips = trips_df[((trips_df.o_zone >= 1000) & (trips_df.o_zone < 2000))| ((trips_df.d_zone >= 1000) & (trips_df.d_zone < 2000))]\n",
    "durham_trips = durham_trips.merge(persons_df,on=[\"household_id\",\"person_id\"])\n",
    "durham_trips = durham_trips[durham_trips.age >=11]\n",
    "durham_trips = durham_trips.merge(zone_map_min,left_on = \"o_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_o\", \"pd_old\":\"pd_old_o\", \"pd_tts\":\"pd_tts_o\"})\n",
    "durham_trips = durham_trips.merge(zone_map_min,left_on = \"d_zone\", right_on =\"DTAZ\").drop(columns = \"DTAZ\").rename(columns = {\"pd_new\":\"pd_new_d\", \"pd_old\":\"pd_old_d\", \"pd_tts\":\"pd_tts_d\"})\n",
    "\n",
    "\n",
    "durham_trips_time = tripmode_df.merge(durham_trips,how = \"right\", on = ['household_id', 'person_id', 'trip_id'])\n",
    "durham_trips_time[\"weight\"] = durham_trips_time[\"weight\"]/10\n",
    "durham_trips_am = durham_trips_time[(durham_trips_time.o_depart >= 360) & (durham_trips_time.o_depart < 540)]\n",
    "durham_trips_pm = durham_trips_time[(durham_trips_time.o_depart >= 900) & (durham_trips_time.o_depart < 1140)]\n",
    "\n",
    "am_modeshare = pd.pivot_table(durham_trips_am,index = [\"pd_tts_o\",\"pd_tts_d\"],columns = \"mode\", values = \"weight\", aggfunc = sum,margins = True).fillna(0)\n",
    "pm_modeshare = pd.pivot_table(durham_trips_pm,index = [\"pd_tts_o\",\"pd_tts_d\"],columns = \"mode\", values = \"weight\", aggfunc = sum,margins = True).fillna(0)\n",
    "am_modeshare.to_csv(os.path.join(output_pth,\"am_modeshare_{}.csv\".format(output_nme)))\n",
    "pm_modeshare.to_csv(os.path.join(output_pth,\"pm_modeshare_{}.csv\".format(output_nme)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "modeller": {
   "desktop_app_port": "64638"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
